{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPtdw28pMo9vLLmLfkvP3wj",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/BNarayanaReddy/CS7015/blob/main/Chapter5_Moment_NAG_Adagrad_RMS_ADAM.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "YVL43LHyHT0Q"
      },
      "outputs": [],
      "source": [
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "height = np.random.normal(loc = 168, scale = 10, size=500) # average 168, deviation 10, size 50\n",
        "weight = np.random.normal(loc = 68, scale = 5, size=500)"
      ],
      "metadata": {
        "id": "wn0WcDoSJQnm"
      },
      "execution_count": 112,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bmi = weight / (height/100)**2"
      ],
      "metadata": {
        "id": "iZl_wzODJQ2Z"
      },
      "execution_count": 113,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "labels = (bmi >= 25).astype(int) # obese = 1 else 0"
      ],
      "metadata": {
        "id": "tcltVmkIJSKV"
      },
      "execution_count": 114,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Network - Multi Label Classification\n",
        "Input (shape: 50, 2) \\\n",
        "      || \\\n",
        "Layer1 (Neurons = 5) \\\n",
        "|| \\\n",
        "Layer 2 (Neurons = 5) \\\n",
        "|| \\\n",
        "Output Layer (Neurons = 1)"
      ],
      "metadata": {
        "id": "7_EqZovJJaYD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "height = height.reshape(-1, 1)\n",
        "weight = weight.reshape(-1, 1)"
      ],
      "metadata": {
        "id": "myD0mJ-0Jb3v"
      },
      "execution_count": 115,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X = np.concatenate([height/np.max(height), weight/np.max(weight)], axis = 1)"
      ],
      "metadata": {
        "id": "LfDFCDXZJdXK"
      },
      "execution_count": 116,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Y = np.copy(labels).reshape(1,-1)"
      ],
      "metadata": {
        "id": "zOBMegEPJesb"
      },
      "execution_count": 117,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B7WgmfTKJgM3",
        "outputId": "afefd968-99d5-4de5-8165-30921346fe06"
      },
      "execution_count": 118,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0.92143501, 0.78773796])"
            ]
          },
          "metadata": {},
          "execution_count": 118
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "Y.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jpISbcrUJhit",
        "outputId": "f2de514a-8409-4364-952d-92246acca4ac"
      },
      "execution_count": 119,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1, 500)"
            ]
          },
          "metadata": {},
          "execution_count": 119
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# weights = [  [<Weights of layer 1, unit 1>, <Weights of layer 1, unit 2>...], [<Weights of layer 2, unit 1>, <Weights of layer 2, unit 2>...]        ]"
      ],
      "metadata": {
        "id": "I3MgIU1ZJl88"
      },
      "execution_count": 120,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def initialize_weights(units, layers = 2, input_dim = 2):\n",
        "  weights = {}\n",
        "  biases = {}\n",
        "  # Hidden layers\n",
        "  for layer in range(1, layers+1):\n",
        "    if layer == 1:\n",
        "      weights[layer] = np.random.randn(units, input_dim) * 0.5\n",
        "      biases[layer] = np.random.randn(1, units) * 0.5\n",
        "    else:\n",
        "      weights[layer] = np.random.randn(units, units) * 0.5\n",
        "      biases[layer] = np.random.randn(1, units) * 0.5\n",
        "\n",
        "  # Output layer\n",
        "  weights[layers+1] = np.random.randn(1, units) * 0.5\n",
        "  biases[layers+1] = np.random.randn(1, 1) * 0.5\n",
        "\n",
        "  return weights, biases"
      ],
      "metadata": {
        "id": "K9pN_vbdJi6b"
      },
      "execution_count": 121,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "weights, biases = initialize_weights(5, 2)"
      ],
      "metadata": {
        "id": "YeNKsFxTJkUa"
      },
      "execution_count": 122,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "weights[1].shape, weights[2].shape, weights[3].shape, biases[1].shape, biases[2].shape, biases[3].shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XNxAj0PhJp7v",
        "outputId": "0702b152-9614-449a-9f8a-be32472f0b99"
      },
      "execution_count": 123,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((5, 2), (5, 5), (1, 5), (1, 5), (1, 5), (1, 1))"
            ]
          },
          "metadata": {},
          "execution_count": 123
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def activation(z, act_fn):\n",
        "  if act_fn == 'relu':\n",
        "    return np.maximum(0,z)\n",
        "  if act_fn == 'sigmoid':\n",
        "    return 1/(1+np.exp(-z))"
      ],
      "metadata": {
        "id": "yxODOsU8Jrne"
      },
      "execution_count": 124,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def forward_prop(X, weights, biases, hidden_activation='relu', output_activation = 'sigmoid'):\n",
        "  # m, n = X.shape\n",
        "  a_op = {}\n",
        "  h_op = {}\n",
        "  h_op[0] = X\n",
        "  # hidden\n",
        "  for i in range(1, len(weights)):\n",
        "    w = weights[i]\n",
        "    b = biases[i]\n",
        "    a_op[i] = np.matmul(h_op[i-1], w.T) + b\n",
        "    h_op[i] = activation(a_op[i], hidden_activation)\n",
        "  # output\n",
        "  a_op[len(weights)] = np.dot(weights[len(weights)], h_op[len(weights)-1].T) + biases[len(weights)]\n",
        "  h_op[len(weights)] = activation(a_op[len(weights)], output_activation)\n",
        "\n",
        "  return a_op, h_op"
      ],
      "metadata": {
        "id": "MTQoECRVJs2k"
      },
      "execution_count": 125,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "a_op, h_op = forward_prop(X, weights, biases)"
      ],
      "metadata": {
        "id": "oHZjNcuMJuDl"
      },
      "execution_count": 126,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_activation_gradient(a, activ_fn):\n",
        "  grad = np.zeros(a.shape)\n",
        "  if activ_fn == 'relu':\n",
        "    grad[a > 0] = 1\n",
        "  if activ_fn == 'sigmoid':\n",
        "    return a*(1-a)\n",
        "  return grad"
      ],
      "metadata": {
        "id": "9HUMABLMJvNK"
      },
      "execution_count": 127,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def backpropagation(X, Y, weights, biases, y_pred, output_activation='sigmoid', hidden_activation='relu'):\n",
        "  a_op, h_op = y_pred\n",
        "  op_layer = 3\n",
        "  output_gradient = h_op[op_layer] - Y\n",
        "\n",
        "  grad_w = {}\n",
        "  grad_b = {}\n",
        "\n",
        "  for layer in range(op_layer, 0, -1):\n",
        "    # print(\"Layer: \", layer)\n",
        "    grad_w[layer] = np.dot(output_gradient, h_op[layer-1])\n",
        "    grad_b[layer] = np.sum(output_gradient, axis=1) # 1, 10\n",
        "\n",
        "    hidden_grad = np.dot(weights[layer].T, output_gradient)\n",
        "\n",
        "    prev_op = a_op[layer]\n",
        "    aggregate_grad = hidden_grad * compute_activation_gradient(h_op[layer-1].T, hidden_activation)\n",
        "\n",
        "\n",
        "    output_gradient = aggregate_grad\n",
        "\n",
        "  return grad_w, grad_b"
      ],
      "metadata": {
        "id": "J2eD5G56Jwc4"
      },
      "execution_count": 128,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_cost(Y_pred, Y):\n",
        "  return np.mean(-Y*np.log10(Y_pred + 1e-8)-(1-Y)*np.log10(1-Y_pred + 1e-8))"
      ],
      "metadata": {
        "id": "DIJMjk5cJyll"
      },
      "execution_count": 129,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def vanilla_gd(X, Y, weights, biases, epochs = 100, lr = 1e-3, output_activation='sigmoid', hidden_activation='relu'):\n",
        "  for epoch in range(epochs):\n",
        "    y_pred = forward_prop(X, weights, biases, hidden_activation, output_activation)\n",
        "\n",
        "    grad_w, grad_b = backpropagation(X, Y, weights, biases, y_pred, output_activation, hidden_activation)\n",
        "\n",
        "    for layer in range(1, len(weights)+1):\n",
        "      weights[layer] -= lr*grad_w[layer]\n",
        "      # print(grad_b[layer].shape)\n",
        "      biases[layer] -= lr*grad_b[layer]\n",
        "\n",
        "  y_pred = forward_prop(X, weights, biases, hidden_activation, output_activation)\n",
        "  gd_cost = compute_cost(y_pred[1][3], Y)\n",
        "  return weights, biases, gd_cost"
      ],
      "metadata": {
        "id": "IKpjk1wNJ2gd"
      },
      "execution_count": 130,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "weights, biases = initialize_weights(5, 2)\n",
        "vanilla_gd(X, Y, weights, biases)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IN3vN0YYNSHx",
        "outputId": "94b53baf-9a66-4c7c-bb04-539b2accb408"
      },
      "execution_count": 131,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "({1: array([[ 0.43698617, -0.07581409],\n",
              "         [ 0.65381624, -0.06136731],\n",
              "         [-1.01563952,  0.07041402],\n",
              "         [-0.58471875, -0.08562356],\n",
              "         [ 0.04171964,  0.36163176]]),\n",
              "  2: array([[-0.20219152, -0.72614578, -0.30531518, -0.00924112,  0.18379936],\n",
              "         [ 0.30440054,  0.46828597, -0.06115064, -0.32691393, -0.36912133],\n",
              "         [-0.94820832, -0.53875948, -0.17020645, -0.00293365,  0.14781074],\n",
              "         [-0.32083909,  0.77151213, -1.08870002, -0.28345569, -0.83709831],\n",
              "         [-0.91551348,  0.26636017,  0.41453549,  0.10179388, -0.23694052]]),\n",
              "  3: array([[ 0.2727478 ,  0.07895604, -0.05008925,  0.01951893,  0.18592804]])},\n",
              " {1: array([[ 0.76741771,  0.52582109,  0.0968249 , -0.05451993,  1.07737662]]),\n",
              "  2: array([[ 0.15816034, -0.36983766, -0.83614025,  0.40669893, -0.39424499]]),\n",
              "  3: array([[-0.49804249]])},\n",
              " np.float64(0.28797047169854556))"
            ]
          },
          "metadata": {},
          "execution_count": 131
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def moment_gd(X, Y, weights, biases, epochs = 100, lr = 1e-3, output_activation='sigmoid', hidden_activation='relu', gamma = 0.98):\n",
        "  for epoch in range(epochs):\n",
        "    y_pred = forward_prop(X, weights, biases, hidden_activation, output_activation)\n",
        "\n",
        "    grad_w, grad_b = backpropagation(X, Y, weights, biases, y_pred, output_activation, hidden_activation)\n",
        "\n",
        "\n",
        "    update_w = {}\n",
        "    update_b = {}\n",
        "\n",
        "    for layer in range(1, len(weights)+1):\n",
        "      update_w[layer] = np.zeros((weights[layer].shape))\n",
        "      update_b[layer] = np.zeros((biases[layer].shape))\n",
        "\n",
        "    for layer in range(1, len(weights)+1):\n",
        "\n",
        "      update_w[layer] = (gamma*update_w[layer]) + lr*grad_w[layer]\n",
        "      update_b[layer] = (gamma*update_b[layer]) + lr*grad_b[layer]\n",
        "\n",
        "      weights[layer] -= update_w[layer]\n",
        "      biases[layer] -= update_b[layer]\n",
        "\n",
        "  y_pred = forward_prop(X, weights, biases, hidden_activation, output_activation)\n",
        "  mgd_cost = compute_cost(y_pred[1][3], Y)\n",
        "  return weights, biases, mgd_cost"
      ],
      "metadata": {
        "id": "TilTRDBsKXvZ"
      },
      "execution_count": 132,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "weights, biases = initialize_weights(5, 2)\n",
        "moment_gd(X, Y, weights, biases, gamma = 0.99)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7pcLthUML52d",
        "outputId": "aaea8958-954b-4ed9-d391-7770afd59c20"
      },
      "execution_count": 133,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "({1: array([[-0.4301708 , -0.55158947],\n",
              "         [ 0.37868323,  0.13363311],\n",
              "         [ 1.14082322,  0.30786474],\n",
              "         [ 0.05590133,  0.0542555 ],\n",
              "         [-0.12511918,  0.1090524 ]]),\n",
              "  2: array([[ 0.47443898, -0.23419657,  0.07998146,  0.77726405, -0.63848996],\n",
              "         [-0.04783648, -0.64972155,  0.23788968, -0.7751783 ,  0.6997581 ],\n",
              "         [ 1.32025805, -0.01048379,  0.69633826,  0.0693517 ,  0.15249587],\n",
              "         [-0.21312267, -0.32422692,  0.21684579, -0.53144651, -0.05994411],\n",
              "         [-1.33016147, -0.12086501, -0.68811105, -0.44488353,  0.04638416]]),\n",
              "  3: array([[-0.98233767,  0.16582054, -0.22151506,  0.3348588 ,  0.3152293 ]])},\n",
              " {1: array([[-0.47869285,  0.10894568, -0.2701648 ,  0.14631273, -0.20568637]]),\n",
              "  2: array([[-0.46452029, -0.95221885,  0.17964414, -0.33193775, -0.07395891]]),\n",
              "  3: array([[-0.32926668]])},\n",
              " np.float64(0.2869972484639911))"
            ]
          },
          "metadata": {},
          "execution_count": 133
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def nesterov_gd(X, Y, weights, biases, epochs = 100, lr = 1e-3, output_activation='sigmoid', hidden_activation='relu', gamma = 0.98):\n",
        "  for epoch in range(epochs):\n",
        "    y_pred = forward_prop(X, weights, biases, hidden_activation, output_activation)\n",
        "\n",
        "    update_w = {}\n",
        "    update_b = {}\n",
        "    w_lookahead = {}\n",
        "    b_lookahead = {}\n",
        "    for layer in range(1, len(weights)+1):\n",
        "      update_w[layer] = np.zeros((weights[layer].shape))\n",
        "      update_b[layer] = np.zeros((biases[layer].shape))\n",
        "\n",
        "\n",
        "    for layer in range(1, len(weights)+1):\n",
        "      w_lookahead[layer] = weights[layer] - gamma*update_w[layer]\n",
        "      b_lookahead[layer] = biases[layer] - gamma*update_b[layer]\n",
        "\n",
        "\n",
        "    grad_w, grad_b = backpropagation(X, Y, w_lookahead, b_lookahead, y_pred, output_activation, hidden_activation)\n",
        "\n",
        "    for layer in range(1, len(weights)+1):\n",
        "      update_w[layer] = (gamma*update_w[layer]) + lr*grad_w[layer]\n",
        "      update_b[layer] = (gamma*update_b[layer]) + lr*grad_b[layer]\n",
        "\n",
        "      weights[layer] -= update_w[layer] + lr*grad_w[layer]\n",
        "      biases[layer] -= update_b[layer] + lr*grad_b[layer]\n",
        "\n",
        "  y_pred = forward_prop(X, weights, biases, hidden_activation, output_activation)\n",
        "  mgd_cost = compute_cost(y_pred[1][3], Y)\n",
        "  return weights, biases, mgd_cost"
      ],
      "metadata": {
        "id": "vSGJEJOWMAqI"
      },
      "execution_count": 134,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "weights, biases = initialize_weights(5, 2)\n",
        "nesterov_gd(X, Y, weights, biases, gamma = 0.99)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Nhri3G6cNkba",
        "outputId": "318d980e-08e7-4a05-d7d5-9c5fc448eae2"
      },
      "execution_count": 135,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "({1: array([[ 0.32716012,  0.12732854],\n",
              "         [ 1.71993214, -0.99959609],\n",
              "         [-0.14650644, -1.08317801],\n",
              "         [ 0.19497428, -0.67370671],\n",
              "         [-0.28660563,  0.3036672 ]]),\n",
              "  2: array([[-0.12420708, -0.6672138 ,  0.44149706,  0.37627987,  0.15834825],\n",
              "         [ 0.03028567, -1.19195039,  0.79172105,  0.07388626, -0.00695355],\n",
              "         [ 0.29206552,  1.04236548, -0.3714885 , -0.49928835, -0.59093016],\n",
              "         [ 0.17557186, -0.50279375, -0.48661442, -0.09009045, -0.09720017],\n",
              "         [-0.25552899,  0.11931919, -0.50446871,  0.85182566,  0.04728329]]),\n",
              "  3: array([[ 0.61890209,  0.9145727 , -0.86258011, -0.99250901, -0.13594265]])},\n",
              " {1: array([[ 0.03885869,  0.35335083, -0.09083193, -0.19514739,  0.13772654]]),\n",
              "  2: array([[ 0.4345529 ,  0.13090139,  0.06790481, -0.31575656,  0.40818658]]),\n",
              "  3: array([[-0.52806561]])},\n",
              " np.float64(0.32692971326014914))"
            ]
          },
          "metadata": {},
          "execution_count": 135
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def adagrad(X, Y, weights, biases, epochs = 100, lr = 1e-3, output_activation='sigmoid', hidden_activation='relu'):\n",
        "  for epoch in range(epochs):\n",
        "    y_pred = forward_prop(X, weights, biases, hidden_activation, output_activation)\n",
        "\n",
        "    update_w = {}\n",
        "    update_b = {}\n",
        "\n",
        "    for layer in range(1, len(weights)+1):\n",
        "      update_w[layer] = np.zeros((weights[layer].shape))\n",
        "      update_b[layer] = np.zeros((biases[layer].shape))\n",
        "\n",
        "\n",
        "    grad_w, grad_b = backpropagation(X, Y, weights, biases, y_pred, output_activation, hidden_activation)\n",
        "\n",
        "    for layer in range(1, len(weights)+1):\n",
        "      update_w[layer] += (grad_w[layer])**2\n",
        "      update_b[layer] += (grad_b[layer])**2\n",
        "\n",
        "      weights[layer] -= (lr/((update_w[layer] + 1e-8)**0.5)) * grad_w[layer]\n",
        "      biases[layer] -= (lr/((update_b[layer] + 1e-8)**0.5)) * grad_b[layer]\n",
        "\n",
        "  y_pred = forward_prop(X, weights, biases, hidden_activation, output_activation)\n",
        "  adagrad_cost = compute_cost(y_pred[1][3], Y)\n",
        "  return weights, biases, adagrad_cost"
      ],
      "metadata": {
        "id": "c-WNsubnQp2T"
      },
      "execution_count": 136,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "weights, biases = initialize_weights(5, 2)\n",
        "adagrad(X, Y, weights, biases)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iDF4dqUWRzFa",
        "outputId": "415eff24-86a1-470e-e9a5-2af19d286416"
      },
      "execution_count": 137,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "({1: array([[-0.24714276,  0.04486096],\n",
              "         [ 0.47431511,  0.3406276 ],\n",
              "         [ 0.18579491, -0.66803038],\n",
              "         [ 0.35642939,  0.19006158],\n",
              "         [ 0.10550974, -0.33215126]]),\n",
              "  2: array([[-0.30564955, -0.2428932 ,  0.88481615,  0.88593697,  0.02598832],\n",
              "         [-0.66590332,  0.59485525,  0.1485907 , -0.42371483,  0.46601335],\n",
              "         [-0.8822107 , -0.47016569, -0.30239002, -0.7136859 ,  0.08422289],\n",
              "         [-0.20657334, -0.34753255,  0.34640142,  0.44832596,  0.29316841],\n",
              "         [ 0.52863577,  0.19090321, -0.05828584, -0.12014932, -0.56198489]]),\n",
              "  3: array([[-0.17917926, -1.11243024, -0.68306875,  0.48032388, -0.77428947]])},\n",
              " {1: array([[0.18235215, 0.64262175, 0.45484002, 0.19914385, 0.08790305]]),\n",
              "  2: array([[ 0.38490212,  0.17196445, -0.55862831, -0.01584433, -0.6116621 ]]),\n",
              "  3: array([[1.04542321]])},\n",
              " np.float64(0.3071612581418124))"
            ]
          },
          "metadata": {},
          "execution_count": 137
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def rmsprop(X, Y, weights, biases, epochs = 100, lr = 1e-3, output_activation='sigmoid', hidden_activation='relu', beta = 0.9):\n",
        "  for epoch in range(epochs):\n",
        "    y_pred = forward_prop(X, weights, biases, hidden_activation, output_activation)\n",
        "\n",
        "    update_w = {}\n",
        "    update_b = {}\n",
        "\n",
        "    for layer in range(1, len(weights)+1):\n",
        "      update_w[layer] = np.zeros((weights[layer].shape))\n",
        "      update_b[layer] = np.zeros((biases[layer].shape))\n",
        "\n",
        "\n",
        "    grad_w, grad_b = backpropagation(X, Y, weights, biases, y_pred, output_activation, hidden_activation)\n",
        "\n",
        "    for layer in range(1, len(weights)+1):\n",
        "      update_w[layer] = beta*update_w[layer] + (1-beta)*(grad_w[layer])**2\n",
        "      update_b[layer] = beta*update_b[layer] + (1-beta)*(grad_b[layer])**2\n",
        "\n",
        "      weights[layer] -= (lr/((update_w[layer] + 1e-8)**0.5)) * grad_w[layer]\n",
        "      biases[layer] -= (lr/((update_b[layer] + 1e-8)**0.5)) * grad_b[layer]\n",
        "\n",
        "  y_pred = forward_prop(X, weights, biases, hidden_activation, output_activation)\n",
        "  rmsprop_cost = compute_cost(y_pred[1][3], Y)\n",
        "  return weights, biases, rmsprop_cost"
      ],
      "metadata": {
        "id": "YGRphrUsR49N"
      },
      "execution_count": 138,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "weights, biases = initialize_weights(5, 2)\n",
        "rmsprop(X, Y, weights, biases, beta = 0.99)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fPQPNGInShwp",
        "outputId": "d4328ae9-85b9-407b-894f-195e92623004"
      },
      "execution_count": 139,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "({1: array([[ 8.11074857e-02, -8.40230836e-01],\n",
              "         [-1.01824732e+00,  7.17165934e-01],\n",
              "         [ 1.35179868e+00, -1.16162592e+00],\n",
              "         [-8.07099957e-01, -2.75056631e-01],\n",
              "         [-4.27292892e-04, -2.90621713e-01]]),\n",
              "  2: array([[-0.50529966,  0.55448097,  1.14623451,  0.94424462, -0.29454386],\n",
              "         [-0.00153593,  1.12934986, -1.07924576,  0.00726724,  0.28518244],\n",
              "         [-0.107936  , -0.77886482,  0.29215288,  0.4478823 ,  0.10839842],\n",
              "         [-0.67181038,  1.20500197, -1.21965356,  0.07379449, -0.64140427],\n",
              "         [-0.45626023,  0.00663744, -0.31320651,  0.80786187,  0.42224474]]),\n",
              "  3: array([[ 0.50758493,  1.10217295, -0.32353367,  1.68657993,  0.03872414]])},\n",
              " {1: array([[-0.52884034,  0.48751937, -0.00062787, -0.44264814, -0.18266504]]),\n",
              "  2: array([[-0.65272271,  0.25237377, -0.32131012,  0.25332547, -0.43918043]]),\n",
              "  3: array([[-1.08568802]])},\n",
              " np.float64(0.2216368953353209))"
            ]
          },
          "metadata": {},
          "execution_count": 139
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def ADAM(X, Y, weights, biases, epochs = 100, lr = 1e-3, output_activation='sigmoid', hidden_activation='relu', beta1 = 0.9, beta2 = 0.99):\n",
        "  for epoch in range(epochs):\n",
        "    y_pred = forward_prop(X, weights, biases, hidden_activation, output_activation)\n",
        "\n",
        "    lr_w = {}\n",
        "    lr_b = {}\n",
        "\n",
        "    moment_w = {}\n",
        "    moment_b = {}\n",
        "\n",
        "    for layer in range(1, len(weights)+1):\n",
        "      moment_w[layer] = np.zeros((weights[layer].shape))\n",
        "      moment_b[layer] = np.zeros((biases[layer].shape))\n",
        "\n",
        "    for layer in range(1, len(weights)+1):\n",
        "      lr_w[layer] = np.zeros((weights[layer].shape))\n",
        "      lr_b[layer] = np.zeros((biases[layer].shape))\n",
        "\n",
        "\n",
        "    grad_w, grad_b = backpropagation(X, Y, weights, biases, y_pred, output_activation, hidden_activation)\n",
        "\n",
        "    for layer in range(1, len(weights)+1):\n",
        "\n",
        "      momentum_bias_correction = 1 - beta1**(epoch+1)\n",
        "      lr_correction = 1 - beta2**(epoch+1)\n",
        "\n",
        "      # moment\n",
        "      moment_w[layer] = (beta1*moment_w[layer] + (1-beta1)*grad_w[layer])/momentum_bias_correction\n",
        "      moment_b[layer] = (beta1*moment_b[layer] + (1-beta1)*grad_b[layer])/momentum_bias_correction\n",
        "\n",
        "      # LR - Adaption\n",
        "      lr_w[layer] = (beta2*lr_w[layer] + (1-beta2)*(grad_w[layer])**2)/lr_correction\n",
        "      lr_b[layer] = (beta2*lr_b[layer] + (1-beta2)*(grad_b[layer])**2)/lr_correction\n",
        "\n",
        "      # Update\n",
        "      weights[layer] -= (lr/((lr_w[layer] + 1e-8)**0.5)) * moment_w[layer]\n",
        "      biases[layer] -= (lr/((lr_b[layer] + 1e-8)**0.5)) * moment_b[layer]\n",
        "\n",
        "  y_pred = forward_prop(X, weights, biases, hidden_activation, output_activation)\n",
        "  adam_cost = compute_cost(y_pred[1][3], Y)\n",
        "  return weights, biases, adam_cost"
      ],
      "metadata": {
        "id": "sOaafiAFSmuJ"
      },
      "execution_count": 140,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "weights, biases = initialize_weights(5, 2)\n",
        "ADAM(X, Y, weights, biases, beta1 = 0.91, beta2 = 0.99)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iLMxrX2VUXgs",
        "outputId": "ab72a562-8fb6-4e39-ba9c-e7e9fbd71eb9"
      },
      "execution_count": 141,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "({1: array([[ 0.62261371,  0.15493885],\n",
              "         [ 0.18368862,  0.5555347 ],\n",
              "         [-1.08075172, -0.59681157],\n",
              "         [-0.2026865 , -0.15525941],\n",
              "         [-0.3467897 ,  0.55655151]]),\n",
              "  2: array([[ 0.72727929,  0.21529042, -0.24563052, -0.48860513, -0.44193537],\n",
              "         [ 0.18325049, -0.09992685, -0.19680009,  0.30027889, -0.5575378 ],\n",
              "         [-0.45370309,  1.22847173,  0.27170622, -0.18823191,  0.31375255],\n",
              "         [-0.06430194, -0.77282963,  1.21199431,  0.26673197,  0.22424152],\n",
              "         [ 0.48440491, -0.02766997, -0.360063  ,  0.38303944,  1.15805186]]),\n",
              "  3: array([[-1.4008536 , -1.40197882, -0.86294694,  0.38610047,  0.49899965]])},\n",
              " {1: array([[ 0.50363692, -0.15453439, -0.8356804 , -0.04716253, -0.46264989]]),\n",
              "  2: array([[-0.26073063, -0.14691104, -0.15567666,  0.3212611 ,  1.21416471]]),\n",
              "  3: array([[-0.39607107]])},\n",
              " np.float64(0.2845596105541566))"
            ]
          },
          "metadata": {},
          "execution_count": 141
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Result Format: Weights, Biases, Cost"
      ],
      "metadata": {
        "id": "YtRIe2r0Ue1M"
      },
      "execution_count": 142,
      "outputs": []
    }
  ]
}
