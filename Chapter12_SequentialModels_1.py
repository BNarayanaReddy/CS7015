# -*- coding: utf-8 -*-
"""Chapter12_SequentialModels-1.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1R_rj560AdDuT3AH-ni7Q8QjQFVJkWQnY
"""

import torch
import numpy as np
import torchaudio
import torch.nn as nn
import transformers

class RNN(nn.Module):
    def __init__(self, input_dim,  hidden_dim, output_dim): # output_dim can be vocab size
      super().__init__()
      self.input_dim =input_dim
      self.hidden_size = hidden_dim


      self.hidden_state = nn.Linear(input_dim + hidden_dim, hidden_dim)
      self.output_trans = nn.Linear(input_dim + hidden_dim, output_dim)
      self.sigmoid = nn.Sigmoid()
      self.softmax = nn.Softmax(dim=1)

    def forward(self, x, init_hidden):
      x = torch.cat((x, init_hidden), dim=1)
      h_out = self.hidden_state(x) # h = w_h@(x,h_0) + b
      hidden_state = self.sigmoid(h_out)
      out = self.output_trans(x) # o = w_u@(x,h) + b
      out_probs = self.softmax(out)
      return out_probs, hidden_state

data = "I am an aspiring AI Engineer"

from huggingface_hub import notebook_login
notebook_login()

from transformers import AutoModel, AutoTokenizer
tokenizer = AutoTokenizer.from_pretrained('google-bert/bert-base-uncased')
emb_model = AutoModel.from_pretrained('google-bert/bert-base-uncased')

tokens = tokenizer.tokenize(data)

len(tokens) # vocab size
# tokens

token_ids = tokenizer.convert_tokens_to_ids(tokens)
# token_ids

# convert to embeddings
token_ids = torch.tensor(token_ids).unsqueeze(dim=0)

token_ids.shape

input_embeddings = emb_model(token_ids)

input_embeddings.last_hidden_state

input_embeddings = input_embeddings.last_hidden_state.detach()

input_embeddings.shape

ce_loss = nn.CrossEntropyLoss()

def fit_rnn_model(vocab_size, hidden_dim=256, input_emb_dim = 768, epochs = 10, lr = 1e-4, eps = 1e-8):

  rnn_model = RNN(input_emb_dim, hidden_dim, vocab_size)
  optimizer = torch.optim.Adam(rnn_model.parameters(), lr=lr, eps=eps)

  for epoch in range(1, epochs):
    optimizer.zero_grad()

    # randomly mask the input embeddings
    output_idx = np.random.randint(0, vocab_size)
    input_embed = input_embeddings.clone()
    input_embed[:, output_idx] = torch.zeros(1, input_emb_dim)

    hidden_state = torch.zeros(1, hidden_dim)

    for tensor in input_embed[0]:
      # teacher forced training
      output, hidden_state = rnn_model(tensor.unsqueeze(dim=0), hidden_state.detach())

    # After applying forward of all the tokens

    loss = ce_loss(output, torch.tensor(output_idx).unsqueeze(0))
    loss.backward()

    optimizer.step()
    if epoch % 10 == 0:
      print(f"Epoch: {epoch}, Loss: {loss.item()}, Masked Token = {tokens[output_idx]} Output Token: {tokens[torch.argmax(output)]}")

  return output, loss.item()

output, loss = fit_rnn_model(6, epochs = 100)

